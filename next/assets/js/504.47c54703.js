(window.webpackJsonp=window.webpackJsonp||[]).push([[504],{2086:function(e,t,o){e.exports=o.p+"assets/img/FacetrackerCLM.b7031e63.jpg"},534:function(e,t,o){"use strict";o.r(t);var i=o(22),r=Object(i.a)({},(function(){var e=this,t=e.$createElement,i=e._self._c||t;return i("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[i("h1",{attrs:{id:"facetracker-clm"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#facetracker-clm"}},[e._v("#")]),e._v(" Facetracker CLM")]),e._v(" "),i("h3",{attrs:{id:"component-type-sensor-subcategory-computer-vision"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#component-type-sensor-subcategory-computer-vision"}},[e._v("#")]),e._v(" Component Type: Sensor (Subcategory: Computer Vision)")]),e._v(" "),i("p",[e._v("The FacetrackerCLM component is one of the vision based plugin. At this stage of development it offers the same functionalities for mouse emulation as the other sensor plugin named "),i("strong",[i("a",{attrs:{href:"FacetrackerLK.htm"}},[e._v("facetrackerLK")])]),e._v(" .")]),e._v(" "),i("p",[e._v("The underlying mechanism is based on the tracking of facial features detected in a first initialization stage by fitting a deformable face model on the image as soon as the region of interest is identified thanks to the OpenCV implementation of the Viola-Jones classifier.")]),e._v(" "),i("p",[e._v("The plugin outputs at each frame the relative offsets of a series of measures based on the tracked points with respect to the previous frame. These offsets may be integrated or used directly as inputs for the mouse emulator actuators. Usually it is a good idea to use the relative displacement of the centre of the face (PosX and PosY) to guide the mouse movements and reserve the other measurements or events to implement other optional functionalities (the head roll, pitch and yaw angles and relative scale of the face).")]),e._v(" "),i("p",[e._v("The FacetrackerCLM also introduces the detection of facial gestures that can be then exploited in the ACS models to directly perform actions or trigger events. To this purpose in this version there are two distinct event related outputs:")]),e._v(" "),i("ul",[i("li",[e._v("Detection of the raising of both eyebrows.")]),e._v(" "),i("li",[e._v("Detection of the "),i("em",[e._v("close")]),e._v(" or "),i("em",[e._v("open")]),e._v(" state of each eye.")])]),e._v(" "),i("p",[i("img",{attrs:{src:o(2086),alt:"Screenshot: FacetrackerCLM plugin",title:"Screenshot: FacetrackerCLM plugin"}}),i("br"),e._v("\nFacetrackerCLM plugin")]),e._v(" "),i("h2",{attrs:{id:"requirements"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#requirements"}},[e._v("#")]),e._v(" Requirements")]),e._v(" "),i("p",[e._v("A camera has to be available in the operating system (preferably a consumer USB camera).")]),e._v(" "),i("h2",{attrs:{id:"input-port-description"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#input-port-description"}},[e._v("#")]),e._v(" Input Port Description")]),e._v(" "),i("p",[e._v("There are no input ports for this plugin.")]),e._v(" "),i("h2",{attrs:{id:"output-port-description"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#output-port-description"}},[e._v("#")]),e._v(" Output Port Description")]),e._v(" "),i("ul",[i("li",[i("p",[i("strong",[e._v("Roll[double]:")]),e._v(" The output port Roll outputs the relative change of the roll angle of the head pose (degrees).")])]),e._v(" "),i("li",[i("p",[i("strong",[e._v("Pitch[double]:")]),e._v(" The output port Pitch outputs the relative change of the pitch angle of the head pose (degrees).")])]),e._v(" "),i("li",[i("p",[i("strong",[e._v("Yaw[double]:")]),e._v(" The output port Yaw outputs the relative change of the yaw angle of the head pose (degrees).")])]),e._v(" "),i("li",[i("p",[i("strong",[e._v("PosX[double]:")]),e._v(" The output port PosX outputs the relative displacement of the x coordinate (image coordinates) of the tracked point (approximatively located around the nose).")])]),e._v(" "),i("li",[i("p",[i("strong",[e._v("PosY[double]:")]),e._v(" The output port PosX outputs the relative displacement of the y coordinate (image coordinates) of the tracked point (approximatively located around the nose).")])]),e._v(" "),i("li",[i("p",[i("strong",[e._v("Scale[double]:")]),e._v(" The output port Scale outputs the relative change in scale of the apparent size of the fitted face model in the current image.")])]),e._v(" "),i("li",[i("p",[i("strong",[e._v("EyeLeft[int]:")]),e._v(" The output port EyeLeft outputs 0 if the left eye is opened, 1 if closed in the current image.")])]),e._v(" "),i("li",[i("p",[i("strong",[e._v("EyeRight[int]:")]),e._v(" The output port EyeRight outputs 0 if the right eye is opened, 1 if closed in the current image.")])])]),e._v(" "),i("h2",{attrs:{id:"event-listener-description"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#event-listener-description"}},[e._v("#")]),e._v(" Event Listener Description")]),e._v(" "),i("ul",[i("li",[i("strong",[e._v("initFace:")]),e._v(" Forces reinit of the fitting of deformable model for the face in order to reset the tracking points.")])]),e._v(" "),i("h2",{attrs:{id:"event-trigger-description"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#event-trigger-description"}},[e._v("#")]),e._v(" Event Trigger Description")]),e._v(" "),i("ul",[i("li",[i("strong",[e._v("EyebrowsRaised:")]),e._v(" this event gets raised everytime the plugin detects a specific configuration of the facial landmarks corresponding to a "),i("em",[e._v("surprise")]),e._v(" expression.")])]),e._v(" "),i("h2",{attrs:{id:"properties"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#properties"}},[e._v("#")]),e._v(" Properties")]),e._v(" "),i("ul",[i("li",[i("p",[i("strong",[e._v("cameraSelection [string, combobox selection]:")]),e._v(" this property determines the index of the input camera. Possible values range from “first camera” to “fifth camera”. If only one camera is available in the system, “first camera” is the correct choice.")])]),e._v(" "),i("li",[i("p",[i("strong",[e._v("cameraResolution [string, combobox selection]:")]),e._v(" this selection box provides several standard camera resolutions. Changing the resolution affects accuracy and performance (CPU load of the runtime system). Provided selections include “160x120”, “320x240”, “640x480” and “800x600”. If the selected resolution cannot be delivered by the image acquisition device, the closest matching resolution is chosen automatically by the plugin.")])]),e._v(" "),i("li",[i("p",[i("strong",[e._v("cameraDisplayUpdate [integer]:")]),e._v(" this property allows to select the update rate for the camera display in milliseconds. If “0” milliseconds is chosen, no window for the live-video will be displayed. If “100” is chosen, the live image window will be updated 10 times a second. Please note that this property does not influence the frame rate of the camera nor the processing interval for new camera frames, only the display in the GUI is adjusted.")])]),e._v(" "),i("li",[i("p",[i("strong",[e._v("userName [string]:")]),e._v(" this property informs the plugin about which is the trained model to load. The actual file name is then automatically expanded as: "),i("em",[i("strong",[e._v("userName")]),e._v("_model.xlm")]),e._v(". The file is searched in the the FacetrackerCLM folder inside the plugin “data” folder.")])])])])}),[],!1,null,null,null);t.default=r.exports}}]);